## ã€CUDA-BEVFusionã€‘qat/export-camera.py ä»£ç è§£è¯»

ã€CUDA-BEVFusionã€‘qat/export-camera.pyä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯å°†ä¸€ä¸ªåä¸º'bevfusion_ptq.pth'çš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œæ”¯æŒ INT8 å’Œ FP16 ä¸¤ç§ç²¾åº¦ã€‚
 - Export INT8 model
```bash
python qat/export-camera.py --ckpt=model/resnet50int8/bevfusion_ptq.pth
```

### `qat/export-camera.py
```python
import sys
import warnings
warnings.filterwarnings("ignore")  # å¿½ç•¥æ‰€æœ‰è­¦å‘Šä¿¡æ¯

import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import os  # ç”¨äºå¤„ç†æ–‡ä»¶å’Œç›®å½•è·¯å¾„

import onnx  # ONNXæ ¼å¼æ”¯æŒåº“
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from onnxsim import simplify  # ç”¨äºç®€åŒ–ONNXæ¨¡å‹
from torchpack.utils.config import configs  # ç”¨äºåŠ è½½é…ç½®æ–‡ä»¶
from mmcv import Config  # MMDetectionåº“ä¸­çš„é…ç½®å·¥å…·
from mmdet3d.models import build_model  # ç”¨äºæ„å»º3Dæ£€æµ‹æ¨¡å‹
from mmdet3d.utils import recursive_eval  # ç”¨äºé€’å½’è¯„ä¼°é…ç½®

from torch import nn  # PyTorchä¸­çš„ç¥ç»ç½‘ç»œæ¨¡å—
from pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer  # é‡åŒ–å·¥å…·
import lean.quantize as quantize  # è‡ªå®šä¹‰é‡åŒ–æ¨¡å—

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Export bevfusion model")
    parser.add_argument('--ckpt', type=str, default='qat/ckpt/bevfusion_ptq.pth', help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument('--fp16', action='store_true', help="æ˜¯å¦ä½¿ç”¨FP16ç²¾åº¦")
    args = parser.parse_args()
    return args

class SubclassCameraModule(nn.Module):
    """è‡ªå®šä¹‰ç›¸æœºæ¨¡å—ï¼Œç”¨äºå¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯"""
    def __init__(self, model):
        super(SubclassCameraModule, self).__init__()
        self.model = model  # ä¼ å…¥çš„æ¨¡å‹

    def forward(self, img, depth):
        """å‰å‘ä¼ æ’­å‡½æ•°"""
        B, N, C, H, W = img.size()  # è·å–è¾“å…¥å›¾åƒçš„ç»´åº¦
        img = img.view(B * N, C, H, W)  # å°†å›¾åƒå±•å¹³

        # é€šè¿‡æ¨¡å‹çš„ç›¸æœºç¼–ç å™¨æå–ç‰¹å¾
        feat = self.model.encoders.camera.backbone(img)
        feat = self.model.encoders.camera.neck(feat)
        if not isinstance(feat, torch.Tensor):
            feat = feat[0]  # å¦‚æœç‰¹å¾ä¸æ˜¯å¼ é‡ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 

        BN, C, H, W = map(int, feat.size())
        feat = feat.view(B, int(BN / B), C, H, W)  # é‡æ–°è°ƒæ•´ç‰¹å¾ç»´åº¦

        def get_cam_feats(self, x, d):
            """è·å–ç›¸æœºç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯"""
            B, N, C, fH, fW = map(int, x.shape)
            d = d.view(B * N, *d.shape[2:])
            x = x.view(B * N, C, fH, fW)

            d = self.dtransform(d)  # æ·±åº¦å˜æ¢
            x = torch.cat([d, x], dim=1)  # å°†æ·±åº¦ä¿¡æ¯å’Œå›¾åƒç‰¹å¾æ‹¼æ¥
            x = self.depthnet(x)  # é€šè¿‡æ·±åº¦ç½‘ç»œå¤„ç†

            depth = x[:, : self.D].softmax(dim=1)  # è®¡ç®—æ·±åº¦æƒé‡
            feat = x[:, self.D: (self.D + self.C)].permute(0, 2, 3, 1)  # è°ƒæ•´ç‰¹å¾ç»´åº¦
            return feat, depth
        
        return get_cam_feats(self.model.encoders.camera.vtransform, feat, depth)

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
    args = parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

    model = torch.load(args.ckpt).module  # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    suffix = "int8"  # é»˜è®¤ä½¿ç”¨INT8é‡åŒ–
    if args.fp16:
        suffix = "fp16"  # å¦‚æœä½¿ç”¨FP16ç²¾åº¦ï¼Œæ›´æ”¹åç¼€
        quantize.disable_quantization(model).apply()  # ç¦ç”¨é‡åŒ–
        
    data = torch.load("example-data/example-data.pth")  # åŠ è½½ç¤ºä¾‹æ•°æ®
    img = data["img"].data[0].cuda()  # å°†å›¾åƒæ•°æ®æ”¾åˆ°GPUä¸Š
    points = [i.cuda() for i in data["points"].data[0]]  # å°†ç‚¹äº‘æ•°æ®æ”¾åˆ°GPUä¸Š

    camera_model = SubclassCameraModule(model)  # åˆ›å»ºè‡ªå®šä¹‰ç›¸æœºæ¨¡å—
    camera_model.cuda().eval()  # å°†æ¨¡å‹æ”¾åˆ°GPUä¸Šå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    depth = torch.zeros(len(points), img.shape[1], 1, img.shape[-2], img.shape[-1]).cuda()  # åˆ›å»ºæ·±åº¦å¼ é‡

    downsample_model = model.encoders.camera.vtransform.downsample  # è·å–ä¸‹é‡‡æ ·æ¨¡å‹
    downsample_model.cuda().eval()  # å°†ä¸‹é‡‡æ ·æ¨¡å‹æ”¾åˆ°GPUä¸Šå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    downsample_in = torch.zeros(1, 80, 360, 360).cuda()  # åˆ›å»ºä¸‹é‡‡æ ·è¾“å…¥å¼ é‡

    save_root = f"qat/onnx_{suffix}"  # è®¾ç½®ä¿å­˜è·¯å¾„
    os.makedirs(save_root, exist_ok=True)  # åˆ›å»ºä¿å­˜ç›®å½•

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        camera_backbone_onnx = f"{save_root}/camera.backbone.onnx"  # ç›¸æœºéª¨å¹²ç½‘ç»œçš„ONNXä¿å­˜è·¯å¾„
        camera_vtransform_onnx = f"{save_root}/camera.vtransform.onnx"  # ç›¸æœºå˜æ¢ç½‘ç»œçš„ONNXä¿å­˜è·¯å¾„
        TensorQuantizer.use_fb_fake_quant = True  # å¯ç”¨ä¼ªé‡åŒ–
        torch.onnx.export(
            camera_model,
            (img, depth),
            camera_backbone_onnx,
            input_names=["img", "depth"],
            output_names=["camera_feature", "camera_depth_weights"],
            opset_version=13,
            do_constant_folding=True,
        )  # å¯¼å‡ºç›¸æœºéª¨å¹²ç½‘ç»œä¸ºONNXæ ¼å¼

        onnx_orig = onnx.load(camera_backbone_onnx)  # åŠ è½½å¯¼å‡ºçš„ONNXæ¨¡å‹
        onnx_simp, check = simplify(onnx_orig)  # ç®€åŒ–ONNXæ¨¡å‹
        assert check, "Simplified ONNX model could not be validated"  # æ£€æŸ¥ç®€åŒ–åçš„æ¨¡å‹æ˜¯å¦æœ‰æ•ˆ
        onnx.save(onnx_simp, camera_backbone_onnx)  # ä¿å­˜ç®€åŒ–åçš„ONNXæ¨¡å‹
        print(f"ğŸš€ The export is completed. ONNX save as {camera_backbone_onnx} ğŸ¤—, Have a nice day~")

        torch.onnx.export(
            downsample_model,
            downsample_in,
            camera_vtransform_onnx,
            input_names=["feat_in"],
            output_names=["feat_out"],
            opset_version=13,
            do_constant_folding=True,
        )  # å¯¼å‡ºç›¸æœºå˜æ¢ç½‘ç»œä¸ºONNXæ ¼å¼
        print(f"ğŸš€ The export is completed. ONNX save as {camera_vtransform_onnx} ğŸ¤—, Have a nice day~")

if __name__ == "__main__":
    main()  # è¿è¡Œä¸»å‡½æ•°
```

### ä»£ç åŠŸèƒ½æ¦‚è¿°ï¼š
- è¯¥ä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯å°†ä¸€ä¸ªåä¸º `bevfusion` çš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œæ”¯æŒ INT8 å’Œ FP16 ä¸¤ç§ç²¾åº¦ã€‚
- ä»£ç é¦–å…ˆè§£æå‘½ä»¤è¡Œå‚æ•°ï¼ŒåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨ FP16 ç²¾åº¦ã€‚
- ç„¶åï¼Œä»£ç åŠ è½½ç¤ºä¾‹æ•°æ®ï¼Œå¹¶å°†æ¨¡å‹å’Œæ•°æ®æ”¾åˆ° GPU ä¸Šã€‚
- æ¥ç€ï¼Œä»£ç å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„ç›¸æœºæ¨¡å— `SubclassCameraModule`ï¼Œç”¨äºå¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯ã€‚
- æœ€åï¼Œä»£ç ä½¿ç”¨ `torch.onnx.export` å°†æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œå¹¶å¯¹å¯¼å‡ºçš„æ¨¡å‹è¿›è¡Œç®€åŒ–å’Œä¿å­˜ã€‚

### ä¸»è¦æ¨¡å—ï¼š
1. **`parse_args`**: è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„å’Œæ˜¯å¦ä½¿ç”¨ FP16 ç²¾åº¦ã€‚
2. **`SubclassCameraModule`**: è‡ªå®šä¹‰çš„ç›¸æœºæ¨¡å—ï¼Œç”¨äºå¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯ã€‚
3. **`main`**: ä¸»å‡½æ•°ï¼Œè´Ÿè´£åŠ è½½æ¨¡å‹ã€å¤„ç†æ•°æ®ã€å¯¼å‡ºæ¨¡å‹ä¸º ONNX æ ¼å¼ï¼Œå¹¶è¿›è¡Œç®€åŒ–å’Œä¿å­˜ã€‚

### å…³é”®ç‚¹ï¼š
- **é‡åŒ–æ”¯æŒ**: ä»£ç æ”¯æŒ INT8 é‡åŒ–ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ `--fp16` å‚æ•°åˆ‡æ¢åˆ° FP16 ç²¾åº¦ã€‚
- **ONNX å¯¼å‡º**: ä½¿ç”¨ `torch.onnx.export` å°†æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œå¹¶ä½¿ç”¨ `onnxsim` å¯¹æ¨¡å‹è¿›è¡Œç®€åŒ–ã€‚
- **GPU åŠ é€Ÿ**: æ¨¡å‹å’Œæ•°æ®éƒ½è¢«æ”¾åˆ° GPU ä¸Šè¿›è¡Œå¤„ç†ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚